{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Profiling with 5 iterations of Warmup\n",
    "| size   | mode    | device   |   batch_size |   context_length |   d_model |   d_ff |   num_layers |   num_heads |   warmup_iter |   n_iter |    mean_s |       std_s |   peak_mem_mb |\n",
    "|:-------|:--------|:---------|-------------:|-----------------:|----------:|-------:|-------------:|------------:|--------------:|---------:|----------:|------------:|--------------:|\n",
    "| small  | fwd     | cuda     |            4 |              128 |       768 |   3072 |           12 |          12 |             5 |       10 | 0.022593  | 0.000104068 |       550.108 |\n",
    "| small  | fwd+bwd | cuda     |            4 |              128 |       768 |   3072 |           12 |          12 |             5 |       10 | 0.0709633 | 0.00825605  |      1280.44  |\n",
    "| medium | fwd     | cuda     |            4 |              128 |      1024 |   4096 |           24 |          16 |             5 |       10 | 0.0445866 | 0.000133085 |      1712.23  |\n",
    "| medium | fwd+bwd | cuda     |            4 |              128 |      1024 |   4096 |           24 |          16 |             5 |       10 | 0.115326  | 0.0012455   |      3422.45  |\n",
    "| large  | fwd     | cuda     |            4 |              128 |      1280 |   5120 |           36 |          20 |             5 |       10 | 0.0805293 | 0.00250605  |      3910.05  |\n",
    "| large  | fwd+bwd | cuda     |            4 |              128 |      1280 |   5120 |           36 |          20 |             5 |       10 | 0.143583  | 0.0150844   |      7743.89  |\n",
    "| xl     | fwd     | cuda     |            4 |              128 |      1600 |   6400 |           48 |          25 |             5 |       10 | 0.066871  | 2.56107e-05 |      7918.65  |\n",
    "| xl     | fwd+bwd | cuda     |            4 |              128 |      1600 |   6400 |           48 |          25 |             5 |       10 | 0.214561  | 0.000210963 |     15732.7   |\n",
    "| 2.7B   | fwd     | cuda     |            4 |              128 |      2560 |  10240 |           32 |          32 |             5 |       10 | 0.0878882 | 0.000122993 |     13268.7   |\n",
    "| 2.7B   | fwd+bwd | cuda     |            4 |              128 |      2560 |  10240 |           32 |          32 |             5 |       10 | 0.303348  | 0.000622047 |     26302.6   |\n",
    "\n",
    "## No Warmup\n",
    "| size   | mode    | device   |   batch_size |   context_length |   d_model |   d_ff |   num_layers |   num_heads |   warmup_iter |   n_iter |    mean_s |      std_s |   peak_mem_mb |\n",
    "|:-------|:--------|:---------|-------------:|-----------------:|----------:|-------:|-------------:|------------:|--------------:|---------:|----------:|-----------:|--------------:|\n",
    "| small  | fwd     | cuda     |            4 |              128 |       768 |   3072 |           12 |          12 |             0 |       10 | 0.11472   | 0.266369   |       550.108 |\n",
    "| small  | fwd+bwd | cuda     |            4 |              128 |       768 |   3072 |           12 |          12 |             0 |       10 | 0.0773883 | 0.048441   |      1280.44  |\n",
    "| medium | fwd     | cuda     |            4 |              128 |      1024 |   4096 |           24 |          16 |             0 |       10 | 0.04986   | 0.0169794  |      1712.23  |\n",
    "| medium | fwd+bwd | cuda     |            4 |              128 |      1024 |   4096 |           24 |          16 |             0 |       10 | 0.107343  | 0.0113035  |      3422.45  |\n",
    "| large  | fwd     | cuda     |            4 |              128 |      1280 |   5120 |           36 |          20 |             0 |       10 | 0.0407184 | 0.00957906 |      3910.05  |\n",
    "| large  | fwd+bwd | cuda     |            4 |              128 |      1280 |   5120 |           36 |          20 |             0 |       10 | 0.146847  | 0.0254056  |      7743.89  |\n",
    "| xl     | fwd     | cuda     |            4 |              128 |      1600 |   6400 |           48 |          25 |             0 |       10 | 0.0701499 | 0.0105331  |      7918.65  |\n",
    "| xl     | fwd+bwd | cuda     |            4 |              128 |      1600 |   6400 |           48 |          25 |             0 |       10 | 0.220418  | 0.021678   |     15732.7   |\n",
    "| 2.7B   | fwd     | cuda     |            4 |              128 |      2560 |  10240 |           32 |          32 |             0 |       10 | 0.0928832 | 0.0158411  |     13268.7   |\n",
    "| 2.7B   | fwd+bwd | cuda     |            4 |              128 |      2560 |  10240 |           32 |          32 |             0 |       10 | 0.302375  | 0.00282029 |     26303.3   |\n",
    "\n",
    "## 1 iteration Warmup\n",
    "| size   | mode    | device   |   batch_size |   context_length |   d_model |   d_ff |   num_layers |   num_heads |   warmup_iter |   n_iter |    mean_s |       std_s |   peak_mem_mb |\n",
    "|:-------|:--------|:---------|-------------:|-----------------:|----------:|-------:|-------------:|------------:|--------------:|---------:|----------:|------------:|--------------:|\n",
    "| small  | fwd     | cuda     |            4 |              128 |       768 |   3072 |           12 |          12 |             1 |       10 | 0.0263708 | 0.000674041 |       550.108 |\n",
    "| small  | fwd+bwd | cuda     |            4 |              128 |       768 |   3072 |           12 |          12 |             1 |       10 | 0.0700107 | 0.00302113  |      1280.44  |\n",
    "| medium | fwd     | cuda     |            4 |              128 |      1024 |   4096 |           24 |          16 |             1 |       10 | 0.0498092 | 0.000133697 |      1712.23  |\n",
    "| medium | fwd+bwd | cuda     |            4 |              128 |      1024 |   4096 |           24 |          16 |             1 |       10 | 0.107668  | 0.0102769   |      3422.45  |\n",
    "| large  | fwd     | cuda     |            4 |              128 |      1280 |   5120 |           36 |          20 |             1 |       10 | 0.0370991 | 0.000139602 |      3910.05  |\n",
    "| large  | fwd+bwd | cuda     |            4 |              128 |      1280 |   5120 |           36 |          20 |             1 |       10 | 0.140533  | 0.0149137   |      7743.89  |\n",
    "| xl     | fwd     | cuda     |            4 |              128 |      1600 |   6400 |           48 |          25 |             1 |       10 | 0.066688  | 0.000166981 |      7918.65  |\n",
    "| xl     | fwd+bwd | cuda     |            4 |              128 |      1600 |   6400 |           48 |          25 |             1 |       10 | 0.214248  | 0.000345418 |     15732.7   |\n",
    "| 2.7B   | fwd     | cuda     |            4 |              128 |      2560 |  10240 |           32 |          32 |             1 |       10 | 0.0873711 | 0.000188711 |     13268.7   |\n",
    "| 2.7B   | fwd+bwd | cuda     |            4 |              128 |      2560 |  10240 |           32 |          32 |             1 |       10 | 0.301108  | 4.89181e-05 |     26302.6   |\n",
    "\n",
    "## 2 iteration Warmup Step\n",
    "| size   | mode    | device   |   batch_size |   context_length |   d_model |   d_ff |   num_layers |   num_heads |   warmup_iter |   n_iter |    mean_s |       std_s |   peak_mem_mb |\n",
    "|:-------|:--------|:---------|-------------:|-----------------:|----------:|-------:|-------------:|------------:|--------------:|---------:|----------:|------------:|--------------:|\n",
    "| small  | fwd     | cuda     |            4 |              128 |       768 |   3072 |           12 |          12 |             2 |       10 | 0.0127079 | 0.00020617  |       550.108 |\n",
    "| small  | fwd+bwd | cuda     |            4 |              128 |       768 |   3072 |           12 |          12 |             2 |       10 | 0.0678351 | 0.00920366  |      1280.44  |\n",
    "| medium | fwd     | cuda     |            4 |              128 |      1024 |   4096 |           24 |          16 |             2 |       10 | 0.0250819 | 0.000291259 |      1712.23  |\n",
    "| medium | fwd+bwd | cuda     |            4 |              128 |      1024 |   4096 |           24 |          16 |             2 |       10 | 0.108795  | 0.00971464  |      3422.45  |\n",
    "| large  | fwd     | cuda     |            4 |              128 |      1280 |   5120 |           36 |          20 |             2 |       10 | 0.0376591 | 0.000149408 |      3910.05  |\n",
    "| large  | fwd+bwd | cuda     |            4 |              128 |      1280 |   5120 |           36 |          20 |             2 |       10 | 0.140864  | 0.0137292   |      7743.89  |\n",
    "| xl     | fwd     | cuda     |            4 |              128 |      1600 |   6400 |           48 |          25 |             2 |       10 | 0.0670614 | 0.000129    |      7918.65  |\n",
    "| xl     | fwd+bwd | cuda     |            4 |              128 |      1600 |   6400 |           48 |          25 |             2 |       10 | 0.215266  | 0.000679135 |     15732.7   |\n",
    "| 2.7B   | fwd     | cuda     |            4 |              128 |      2560 |  10240 |           32 |          32 |             2 |       10 | 0.0875296 | 0.000115729 |     13268.7   |\n",
    "| 2.7B   | fwd+bwd | cuda     |            4 |              128 |      2560 |  10240 |           32 |          32 |             2 |       10 | 0.302965  | 0.00062237  |     26302.6   |\n",
    "\n",
    "\n",
    "### Why warmup matters:\n",
    "\n",
    "Eliminates one-time initialization costs (CUDA kernel compilation, memory allocation, GPU frequency ramp-up)\n",
    "Reduces variance from 200%+ down to <1% of mean execution time\n",
    "Particularly critical for smaller models where overhead is proportionally larger\n",
    "\n",
    "### Warmup iteration comparison:\n",
    "\n",
    "0 iterations: Extremely high variance (up to 231% relative std)\n",
    "1-2 iterations: Major improvement, ~1-3% variance - acceptable for most cases\n",
    "5 iterations: Optimal stability, <1% variance - best for rigorous benchmarking\n",
    "\n",
    "### Key insight:\n",
    "\n",
    "GPUs need time to reach stable clock frequencies and for CUDA runtime to optimize\n",
    "1-2 warmups handle most initialization, but 5 iterations ensure complete stabilization\n",
    "The small cost of extra warmup iterations pays off with significantly more reliable measurements\n",
    "\n",
    "### bfloat16 autocast with warmup\n",
    "| size   | mode    | device   |   batch_size |   context_length |   d_model |   d_ff |   num_layers |   num_heads |   warmup_iter |   n_iter |    mean_s |       std_s |   peak_mem_mb |\n",
    "|:-------|:--------|:---------|-------------:|-----------------:|----------:|-------:|-------------:|------------:|--------------:|---------:|----------:|------------:|--------------:|\n",
    "| small  | fwd     | cuda     |            4 |              128 |       768 |   3072 |           12 |          12 |             5 |       10 | 0.0139131 | 0.000193304 |       797.132 |\n",
    "| small  | fwd+bwd | cuda     |            4 |              128 |       768 |   3072 |           12 |          12 |             5 |       10 | 0.0791648 | 0.00339954  |      1262.96  |\n",
    "| medium | fwd     | cuda     |            4 |              128 |      1024 |   4096 |           24 |          16 |             5 |       10 | 0.0282443 | 0.000629777 |      2480.99  |\n",
    "| medium | fwd+bwd | cuda     |            4 |              128 |      1024 |   4096 |           24 |          16 |             5 |       10 | 0.123578  | 0.00524961  |      3579.45  |\n",
    "| large  | fwd     | cuda     |            4 |              128 |      1280 |   5120 |           36 |          20 |             5 |       10 | 0.0422451 | 0.000129378 |      5708.23  |\n",
    "| large  | fwd+bwd | cuda     |            4 |              128 |      1280 |   5120 |           36 |          20 |             5 |       10 | 0.162037  | 0.0151045   |      7784.98  |\n",
    "| xl     | fwd     | cuda     |            4 |              128 |      1600 |   6400 |           48 |          25 |             5 |       10 | 0.0556125 | 0.000107493 |     11754     |\n",
    "| xl     | fwd+bwd | cuda     |            4 |              128 |      1600 |   6400 |           48 |          25 |             5 |       10 | 0.216072  | 0.0245934   |     15749.9   |\n",
    "| 2.7B   | fwd     | cuda     |            4 |              128 |      2560 |  10240 |           32 |          32 |             5 |       10 | 0.0383748 | 0.000362194 |     19655     |\n",
    "| 2.7B   | fwd+bwd | cuda     |            4 |              128 |      2560 |  10240 |           32 |          32 |             5 |       10 | 0.149899  | 0.0150371   |     26292.7   |\n",
    "\n",
    "At lower model sizes, overhead is larger than the actual execution, so bf16 does not provide much benefit. But at 2.7B, there is almost 50% improvement in speed, and the gap probably would increase dramatically on even larger models (since FLOPs is a magnitude higher with bf16 than fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0001)\n",
      "tensor(9.9531, dtype=torch.float16)\n",
      "tensor(10.0021)\n",
      "tensor(10.0021)\n"
     ]
    }
   ],
   "source": [
    "s = torch.tensor(0,dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.float32)\n",
    "print(s)\n",
    "s = torch.tensor(0,dtype=torch.float16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.float16)\n",
    "print(s)\n",
    "s = torch.tensor(0,dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.float16)\n",
    "print(s)\n",
    "s = torch.tensor(0,dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    x = torch.tensor(0.01,dtype=torch.float16)\n",
    "    s += x.type(torch.float32)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: Pure float32\n",
    "All operations are in float32 (32-bit precision)\n",
    "Should theoretically be 10.0, but accumulated rounding errors from 1000 additions result in 10.0001\n",
    "Float32 has ~7 decimal digits of precision\n",
    "\n",
    "Case 2: Pure float16\n",
    "All operations are in float16 (16-bit precision, also called \"half precision\")\n",
    "Float16 has only ~3-4 decimal digits of precision\n",
    "Much larger accumulated error: 9.9531 instead of 10.0\n",
    "The value 0.01 cannot be represented exactly in float16, and errors compound quickly\n",
    "\n",
    "Case 3: Mixed Type with automatic promotion\n",
    "Key behavior: PyTorch promotes float16 to float32 during the addition\n",
    "However, 0.01 is first converted to float16 (with precision loss), then promoted to float32\n",
    "This \"bad\" float16 representation is added to the float32 accumulator\n",
    "Result: worse than pure float32, but the accumulator maintains float32 precision\n",
    "\n",
    "Case 4: Same as Case 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Autocast:\n",
      "input x: torch.float32\n",
      "post fc1 x: torch.float32\n",
      "post relu x: torch.float32\n",
      "post ln x: torch.float32\n",
      "post fc2 x: torch.float32\n",
      "grad data type torch.float32\n",
      "With Autocast:\n",
      "input x: torch.float32\n",
      "post fc1 x: torch.float16\n",
      "post relu x: torch.float16\n",
      "post ln x: torch.float32\n",
      "post fc2 x: torch.float16\n",
      "grad data type torch.float32\n",
      "With Autocast bf16:\n",
      "input x: torch.float32\n",
      "post fc1 x: torch.bfloat16\n",
      "post relu x: torch.bfloat16\n",
      "post ln x: torch.float32\n",
      "post fc2 x: torch.bfloat16\n",
      "grad data type torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        print(f\"input x: {x.dtype}\")\n",
    "        x = self.fc1(x)\n",
    "        print(f\"post fc1 x: {x.dtype}\")\n",
    "        x = self.relu(x)\n",
    "        print(f\"post relu x: {x.dtype}\")\n",
    "        x = self.ln(x)\n",
    "        print(f\"post ln x: {x.dtype}\")\n",
    "        x = self.fc2(x)\n",
    "        print(f\"post fc2 x: {x.dtype}\")\n",
    "        return x\n",
    "\n",
    "x = torch.randn(10, dtype=torch.float32, device='mps')\n",
    "model = ToyModel(10, 10).to('mps')\n",
    "model.zero_grad(set_to_none=True)\n",
    "\n",
    "print(\"Without Autocast:\")\n",
    "y = model(x)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "print(f\"grad data type {model.fc1.weight.grad.dtype}\")\n",
    "\n",
    "print(\"With Autocast:\")\n",
    "model.zero_grad(set_to_none=True)\n",
    "with torch.autocast(device_type='mps', dtype=torch.float16):\n",
    "    y=model(x)\n",
    "    loss = y.sum()\n",
    "    loss.backward()\n",
    "    print(f\"grad data type {model.fc1.weight.grad.dtype}\")\n",
    "\n",
    "print(\"With Autocast bf16 (limited support on mps):\")\n",
    "model.zero_grad(set_to_none=True)\n",
    "with torch.autocast(device_type='mps', dtype=torch.bfloat16):\n",
    "    y=model(x)\n",
    "    loss = y.sum()\n",
    "    loss.backward()\n",
    "    print(f\"grad data type {model.fc1.weight.grad.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Types in Mixed Precision Training\n",
    "\n",
    "Model Parameters (within autocast context):\n",
    "FP32 - The original parameters remain in FP32. Autocast doesn't change the stored parameter types; it only casts them temporarily during forward pass operations.\n",
    "\n",
    "Output of fc1 (first linear layer):\n",
    "FP16 - Linear layers are in the autocast \"eligible\" list, so the FP32 weights are cast to FP16, and the computation happens in FP16.\n",
    "\n",
    "Output after ReLU:\n",
    "FP16 - ReLU is a pointwise operation that preserves the input dtype, so it remains FP16.\n",
    "\n",
    "Output of LayerNorm (ln):\n",
    "FP32 - LayerNorm is on the \"FP32 ops\" list because it involves reductions and normalization that benefit from higher precision for numerical stability.\n",
    "\n",
    "Output of fc2 (second linear layer):\n",
    "FP16 - Even though the input is FP32 from LayerNorm, fc2 is an eligible operation, so it gets cast back to FP16 for the matrix multiplication.\n",
    "\n",
    "Model's predicted logits:\n",
    "FP16 - The final output from fc2 remains in FP16.\n",
    "\n",
    "Loss:\n",
    "FP32 - Most loss functions (CrossEntropyLoss, MSELoss, etc.) automatically promote their inputs to FP32 for numerical stability, even within the autocast context.\n",
    "\n",
    "Model's gradients:\n",
    "FP32 - Gradients are accumulated in the same dtype as the parameters, which remain in FP32. During the backward pass, even though some forward activations were FP16, the gradients are computed and accumulated in FP32.\n",
    "\n",
    "What parts of LayerNorm are sensitive to mixed precision?\n",
    "\n",
    "LayerNorm logic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, gamma, beta, eps=1e-5):\n",
    "    # Normalize over last dimension(s)\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
    "    x_norm = (x - mean) / torch.sqrt(var + eps)\n",
    "    \n",
    "    # Apply affine transform\n",
    "    return gamma * x_norm + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance computation, division by small numbers are susceptible to low precision errors. Even grads in these case would involves multiplicationa and division which could cause issues in grads.\n",
    "\n",
    "bfloat16 has a higher range so autocast actually keeps bf16 for LayerNorm. Not true for MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Attention Benchmarks\n",
    "| device   |   batch_size |   d_model |   seq_len |   fw_mean_s |    fw_std_s |   bw_mean_s |    bw_std_s |   mem_before_bwd_mb | status   | error   |\n",
    "|:---------|-------------:|----------:|----------:|------------:|------------:|------------:|------------:|--------------------:|:---------|:--------|\n",
    "| cuda     |            8 |        16 |       256 | 0.00257019  | 0.0216639   |  0.00340066 | 0.0286695   |             69.7744 | ok       |         |\n",
    "| cuda     |            8 |        16 |      1024 | 0.000339281 | 0.000260863 |  0.0011211  | 0.000248008 |            135.095  | ok       |         |\n",
    "| cuda     |            8 |        16 |      4096 | 0.0025646   | 0.000202286 |  0.0062171  | 8.71038e-05 |           1116.38   | ok       |         |\n",
    "| cuda     |            8 |        16 |      8192 | 0.00994167  | 7.48688e-05 |  0.0238352  | 0.000104717 |           4216.75   | ok       |         |\n",
    "| cuda     |            8 |        16 |     16384 | 0.0390492   | 0.00103785  |  0.0940123  | 0.000161966 |          16561.5    | ok       |         |\n",
    "| cuda     |            8 |        16 |     32768 |            |            |            |            |                     | oom      | OOM     |\n",
    "| cuda     |            8 |        32 |       256 | 0.000114554 | 9.4068e-05  |  0.00115204 | 0.00025819  |             71.5244 | ok       |         |\n",
    "| cuda     |            8 |        32 |      1024 | 0.000256942 | 0.000121982 |  0.00114297 | 0.000280624 |            142.095  | ok       |         |\n",
    "| cuda     |            8 |        32 |      4096 | 0.00265978  | 8.06978e-05 |  0.00630066 | 8.19027e-05 |           1144.38   | ok       |         |\n",
    "| cuda     |            8 |        32 |      8192 | 0.0102459   | 6.41021e-05 |  0.0241119  | 8.92101e-05 |           4272.75   | ok       |         |\n",
    "| cuda     |            8 |        32 |     16384 | 0.0400996   | 7.79182e-05 |  0.095068   | 0.000117993 |          16673.5    | ok       |         |\n",
    "| cuda     |            8 |        64 |       256 | 0.000121771 | 0.000129119 |  0.00112999 | 0.000173525 |             75.0244 | ok       |         |\n",
    "| cuda     |            8 |        64 |      1024 | 0.000267888 | 6.20349e-05 |  0.00111495 | 0.000106903 |            156.095  | ok       |         |\n",
    "| cuda     |            8 |        64 |      4096 | 0.00292579  | 7.5545e-05  |  0.00680642 | 8.12444e-05 |           1200.38   | ok       |         |\n",
    "| cuda     |            8 |        64 |      8192 | 0.011448    | 6.54827e-05 |  0.0264497  | 0.000127748 |           4384.75   | ok       |         |\n",
    "| cuda     |            8 |        64 |     16384 | 0.0454684   | 7.17934e-05 |  0.104967   | 0.000115151 |          16897.5    | ok       |         |\n",
    "| cuda     |            8 |        64 |     32768 |            |            |            |            |                     | oom      | OOM     |\n",
    "| cuda     |            8 |       128 |       256 | 0.000127997 | 0.000138762 |  0.0011494  | 0.000274242 |             82.0244 | ok       |         |\n",
    "| cuda     |            8 |       128 |      1024 | 0.000306624 | 6.59696e-05 |  0.00117051 | 0.000109251 |            184.095  | ok       |         |\n",
    "| cuda     |            8 |       128 |      4096 | 0.00356625  | 0.000295742 |  0.00803529 | 9.82754e-05 |           1312.38   | ok       |         |\n",
    "| cuda     |            8 |       128 |      8192 | 0.0137337   | 6.37877e-05 |  0.0308589  | 8.39673e-05 |           4608.75   | ok       |         |\n",
    "| cuda     |            8 |       128 |     16384 | 0.0542224   | 8.8418e-05  |  0.122406   | 0.000125221 |          17345.5    | ok       |         |\n",
    "| cuda     |            8 |       128 |     32768 |            |            |            |            |                     | oom      | OOM     |\n",
    "| cuda     |            8 |       256 |     16384 | 0.0743859   | 5.71176e-05 |  0.163467   | 5.38268e-05 |           18241.5   | ok       |         |\n",
    "| cuda     |            8 |       256 |     32768 |            |            |            |            |                     | oom      | OOM     |\n",
    "| cuda     |            8 |       512 |     32768 |            |            |            |            |                     | oom      | OOM     |\n",
    "| cuda     |            8 |      1024 |     16384 | 0.204574    | 0.000816931 |  0.414481   | 0.000385968 |          23617.5    | ok       |         |\n",
    "| cuda     |            8 |      1024 |     32768 |            |            |            |            |                     | oom      | OOM     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
